# Mini Project!

## **Présentation du projet**

Ce projet vise à construire un pipeline de données complet à partir d’un jeu de données brut. L’objectif est de transformer les données brutes en un schéma adapté pour un **Data Warehouse**, permettant d’effectuer des analyses efficaces. Nous débuterons par une exploration du schéma initial, qui nécessitera une **dénormalisation** afin de répondre aux besoins analytiques spécifiques.

---

### **Contexte**
Le jeu de données utilisé dans ce projet provient d'une entreprise de vente au détail souhaitant optimiser ses opérations commerciales. Ce jeu de données contient des informations détaillées sur plusieurs aspects de l'activité, notamment :

- **Suivi des commandes clients** : Identifier qui achète quoi, quand, et dans quel magasin.
- **Gestion des stocks** : Suivre les produits commandés et gérer les niveaux d'approvisionnement.
- **Analyse des performances des magasins** : Mesurer l'efficacité des magasins en fonction des commandes traitées, des ventes réalisées, et des taux de taxe appliqués.
- **Optimisation des coûts** : Évaluer les coûts des approvisionnements, particulièrement ceux des articles périssables, pour mieux gérer les marges.

L’objectif global pour l’entreprise est de mieux comprendre son activité et de prendre des décisions éclairées concernant :
- La gestion des stocks.
- L’optimisation des prix.
- La maximisation des ventes.

---

### **Objectifs et technologies**
Le projet repose sur l’utilisation de **DBT (Data Build Tool)** pour orchestrer les transformations de données, et d’**Amazon Redshift** pour héberger l’entrepôt de données. Ce cadre technique nous permettra de travailler efficacement sur la transformation et la structuration des données tout en améliorant nos compétences en **SQL** et en modélisation.

#### **Objectifs du projet :**
1. **Créer un schéma adapté pour un Data Warehouse** :
   - Simplifier les requêtes analytiques.
   - Optimiser les performances pour l’analyse.

2. **Implémenter un pipeline de données reproductible** :
   - Transformer les données brutes en tables dénormalisées prêtes à l’analyse.

3. **Améliorer les compétences techniques** :
   - Appliquer les concepts de modélisation des données.
   - Renforcer l'expertise en **ETL (Extract, Transform, Load)** et en **data modeling**.
